{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e903638-dea1-4599-878e-61e3463f831d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.query_engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex, SimpleDirectoryReader\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QueryEmbeddingSearch\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Set up OpenAI API key directly\u001b[39;00m\n\u001b[1;32m      7\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-proj-tVkV-eVv58clQ1y2vyghrxg6U0ibpBonHUmYmTJActRxQEi3DVY5pZ2hMCfVHJQZ4Zr0BHhfY4T3BlbkFJCK9jnzQbJDC9aGvq6ZfYsaamyTOpDlziscL1v42_G5SuaarAUAmNJwHa2AuEkugYgYlUUKhYkA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.query_engine'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Set up OpenAI API key directly\n",
    "openai.api_key = \"\"\n",
    "\n",
    "# Initialize chat history\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Ask me a question about Streamlit's open-source Python library!\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to load data and set up the LLM with LlamaIndex\n",
    "def load_data():\n",
    "    # Read all documents in the specified directory (./data) and load them\n",
    "    reader = SimpleDirectoryReader(input_dir=\"./data\", recursive=True)\n",
    "    docs = reader.load_data()\n",
    "    \n",
    "    # Configure the LLM model with context specifically for the Streamlit library\n",
    "    llm = OpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.2,\n",
    "        system_prompt=\"\"\"You are an expert on \n",
    "        the OTL_edit dataset and your \n",
    "        job is to answer questions about theses books' information. \n",
    "        Assume that all questions are related \n",
    "        to the OTL_edit.cvs and \n",
    "        answer based only on the provided documents.\"\"\",\n",
    "    )\n",
    "    \n",
    "    # Create an index based on the documents loaded from the specified directory\n",
    "    index = VectorStoreIndex.from_documents(docs, llm=llm)\n",
    "    return index\n",
    "\n",
    "# Load data and create a chat engine for interactions based on the indexed data\n",
    "index = load_data()\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True, streaming=True)\n",
    "\n",
    "# Start chat interaction loop\n",
    "print(\"Chat with the Streamlit docs, powered by LlamaIndex ðŸ’¬ðŸ¦™\")\n",
    "print(\"Ask me a question about Streamlit's open-source Python library! Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    prompt = input(\"You: \")\n",
    "    if prompt.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Append user question to messages\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Generate a response based on the indexed documents only\n",
    "    response_stream = chat_engine.stream_chat(prompt)\n",
    "    assistant_response = \"\".join(response_stream.response_gen)  # Collect streamed response\n",
    "    \n",
    "    # Display the assistant's response and save it in chat history\n",
    "    print(\"Assistant:\", assistant_response)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf097d7d-91b5-4ab4-8a1a-f84c6eba4bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
